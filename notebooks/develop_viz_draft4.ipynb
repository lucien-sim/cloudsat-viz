{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop Cloudsat/ERA visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, MultiPolygon, Point, box\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from pyhdf.SD import SD, SDC\n",
    "from pyhdf.HDF import *\n",
    "from pyhdf.VS import *\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "from bokeh.io import show, output_file, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool, CustomJS, LinearColorMapper, LogColorMapper, FixedTicker, ColorBar\n",
    "from bokeh.palettes import Reds6\n",
    "from bokeh.plotting import figure, save\n",
    "from bokeh.resources import CDN\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Panel, Tabs\n",
    "from bokeh.models.widgets import Select\n",
    "from bokeh.models.formatters import DatetimeTickFormatter\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "data_path = os.path.join('/','Users','Lucien','Documents','cloudsat_project','data')\n",
    "figs_path = os.path.join('/','Users','Lucien','Documents','cloudsat_project','figs')\n",
    "default_crs = 'epsg:3995'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================================\n",
    "# Functions to set up plot. \n",
    "\n",
    "def specify_area_of_interest_EPSG4326(bbox=(-180,60,180,90)): \n",
    "    area_of_int = gpd.GeoDataFrame({'geometry':[box(*bbox)]})\n",
    "    area_of_int.crs = {'init':'epsg:4326'}\n",
    "    return area_of_int\n",
    "\n",
    "\n",
    "def specify_area_of_interest_EPSG3995(bbox=(-3e6,-3e6,3e6,3e6)): \n",
    "    area_of_int = gpd.GeoDataFrame({'geometry':[box(*bbox)]})\n",
    "    area_of_int.crs = {'init':default_crs}\n",
    "    return area_of_int\n",
    "\n",
    "\n",
    "def load_country_geometries(area_of_interest):\n",
    "    countries = gpd.read_file(os.path.join(data_path,'Countries_WGS84','Countries_WGS84.shp'))\n",
    "    countries = countries.to_crs({'init':default_crs})\n",
    "    countries = gpd.overlay(countries,area_of_interest,how='intersection')\n",
    "    return countries\n",
    "\n",
    "\n",
    "def prepare_polygon_coords_for_bokeh(countries): \n",
    "    \"\"\"Need to go from a list of points to two lists of lists (one for x and y coordinates). \n",
    "    For each list of lists, the inner lists contain the x or y coordinates for each point in\n",
    "    a single polygon, while the outer list has one element for each polygon. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Simplify shapes (to resolution of 10000 meters), convert polygons to multipolygons. \n",
    "    list_of_polygons = []\n",
    "    for raw_poly in countries['geometry']: \n",
    "        raw_poly = raw_poly.simplify(10000, preserve_topology=False)\n",
    "        if isinstance(raw_poly,Polygon): \n",
    "            raw_poly = MultiPolygon([raw_poly])\n",
    "        for poly in list(raw_poly): \n",
    "            list_of_polygons.append(poly)\n",
    "            \n",
    "    # Create lists of lists. \n",
    "    x_coords = [list(poly.exterior.coords.xy[0]) for poly in list_of_polygons]\n",
    "    y_coords = [list(poly.exterior.coords.xy[1]) for poly in list_of_polygons]\n",
    "    \n",
    "    return x_coords,y_coords\n",
    "\n",
    "\n",
    "#=============================================================================================\n",
    "# Functions for reading data from HDF-EOS files. \n",
    "\n",
    "def read_datasets(FILENAME): \n",
    "    \"\"\"Reads all datasets in the file, places fill_values with NaNs, and places in dictionary.\"\"\"\n",
    "    # Open the file\n",
    "    file = SD(FILENAME, SDC.READ)\n",
    "    datasets_dict = file.datasets()\n",
    "    dataset_names = [dname for dname in datasets_dict.keys()]\n",
    "\n",
    "    # Extract a single dataset from the file, replace the fill values with NaN. \n",
    "    data_dict = {}\n",
    "    for ds in dataset_names: \n",
    "        sds_obj = file.select(ds) \n",
    "        data = sds_obj.get() \n",
    "        fill_val = sds_obj.attributes().get('_FillValue')\n",
    "        if fill_val: \n",
    "            data = data.astype(np.float32)\n",
    "            data[data==fill_val] = np.NaN\n",
    "        data_dict[ds] = data\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def read_vdata(FILENAME): \n",
    "    \"\"\"Reads all vdata fields in the file and places in dictionary.\"\"\"\n",
    "    # Prepare to read the data. \n",
    "    f = HDF(FILENAME, SDC.READ)        # Open the file\n",
    "    vs = f.vstart()                    # Start the vdata interface\n",
    "    data_info_list = vs.vdatainfo()    # List the vdata fields\n",
    "    vdata_fieldnames = [a[0] for a in data_info_list]    # Get the names\n",
    "\n",
    "    # Load the data, place in dictionary\n",
    "    vdata_dict = {}     \n",
    "    for field in vdata_fieldnames: \n",
    "        vdata_dict[field] = np.squeeze(np.asarray(vs.attach(field)[:]))\n",
    "\n",
    "    # terminate the vdata interface, close the file. \n",
    "    vs.end() \n",
    "    f.close()\n",
    "\n",
    "    return vdata_dict\n",
    "\n",
    "\n",
    "def add_datetime(vdata_dict,FILENAME):\n",
    "    \"\"\"Adds a datetime vector (for time of each measurement) to the vdata_dict\"\"\"\n",
    "    first_second = np.around(vdata_dict['UTC_start'],decimals=-2)\n",
    "    first_dtime  = np.asarray(datetime.datetime.strptime(FILENAME.split('/')[-1][:13],'%Y%j%H%M%S'))\\\n",
    "                             .astype('datetime64[D]')+pd.Timedelta(str(first_second)+' seconds')\n",
    "    tv = first_dtime+(np.around(vdata_dict['Profile_time'],decimals=2)*pd.Timedelta('1 seconds'))\n",
    "    time_vec = np.asarray([pd.Timestamp(t) for t in tv])\n",
    "    vdata_dict['datetime'] = time_vec\n",
    "    return vdata_dict\n",
    "\n",
    "\n",
    "def read_cloudsat_file(FILENAME): \n",
    "    \"\"\"Reads an entire CloudSat file (HDF-EOS format)\"\"\"\n",
    "\n",
    "    data_dict = read_datasets(FILENAME)\n",
    "    vdata_dict = read_vdata(FILENAME)\n",
    "    vdata_dict = add_datetime(vdata_dict,FILENAME)\n",
    "      \n",
    "    return data_dict,vdata_dict\n",
    "\n",
    "\n",
    "#=============================================================================================\n",
    "# Functions for reading data from HDF-EOS files. \n",
    "\n",
    "def get_all_downloaded_pass_ids(data_path):\n",
    "    return sorted(list(set([a[:19] for a in os.listdir(os.path.join(data_path,'cloudsat')) if a[0]!='.'])))\n",
    "\n",
    "\n",
    "def create_radarPass_instance(data_path,pass_id):\n",
    "    geoprof_fname = os.path.join(data_path,'cloudsat',\n",
    "                                 pass_id+'_CS_2B-'+'GEOPROF'+'_GRANULE_P1_R05_E06_F00.hdf')\n",
    "    data_dict,vdata_dict = read_cloudsat_file(geoprof_fname)\n",
    "    radar_pass = radarPass(vdata_dict['Longitude'],vdata_dict['Latitude'],\n",
    "                           vdata_dict['datetime'],np.ravel(np.nanmean(data_dict['Height'].T,1)[::-1]),\n",
    "                           np.flipud(data_dict['Radar_Reflectivity'].T),\n",
    "                           np.flipud(data_dict['CPR_Cloud_mask'].T))\n",
    "    return radar_pass\n",
    "\n",
    "\n",
    "def add_cldclass_to_radarPass(radar_pass,data_path,pass_id): \n",
    "    cldclass_fname = os.path.join(data_path,'cloudsat',\n",
    "                                  pass_id+'_CS_2B-'+'CLDCLASS'+'_GRANULE_P1_R05_E06_F00.hdf')\n",
    "    data_dict,vdata_dict = read_cloudsat_file(cldclass_fname)\n",
    "    radar_pass = radar_pass.add_cloudclass(data_dict,vdata_dict)\n",
    "    return radar_pass\n",
    "\n",
    "\n",
    "def add_cwc_to_radarPass(radar_pass,data_path,pass_id):\n",
    "    cwc_fname = os.path.join(data_path,'cloudsat',\n",
    "                             pass_id+'_CS_2B-'+'CWC-RO'+'_GRANULE_P1_R05_E06_F00.hdf')\n",
    "    data_dict,vdata_dict = read_cloudsat_file(cwc_fname)\n",
    "    radar_pass = radar_pass.add_cwc(data_dict,vdata_dict)\n",
    "    return radar_pass\n",
    "\n",
    "\n",
    "def save_radarPass_object_pkl(radar_pass,data_path):\n",
    "    pass_timestamp = radar_pass.cloudsat['timestamp'][len(radar_pass.cloudsat['timestamp'])//2]\\\n",
    "                               .strftime('%Y%m%d_%H%M%S')\n",
    "    fname = 'radarPass_plot_'+pass_timestamp+'.pkl'\n",
    "    with open(os.path.join(data_path,'radar_passes',fname),'wb') as f:\n",
    "        pickle.dump(radar_pass, f, 2)\n",
    "    return pass_timestamp\n",
    "\n",
    "\n",
    "def save_radarPass_object_json(radar_pass,data_path):\n",
    "    pass_timestamp = radar_pass.cloudsat['timestamp'][len(radar_pass.cloudsat['timestamp'])//2]\\\n",
    "                               .strftime('%Y%m%d_%H%M%S')\n",
    "    fname = 'radarPass_plot_'+pass_timestamp+'.json'\n",
    "    radar_dict = radar_pass.get_json_serializable_obj()\n",
    "    radar_dict['timestamp'] = [tstamp.strftime('%Y%m%d_%H%M%S.%f') for tstamp in radar_dict['timestamp']]\n",
    "    with open(os.path.join(data_path,'radar_passes',fname),'w') as f:\n",
    "        json.dump(radar_dict, f)\n",
    "    return pass_timestamp\n",
    "\n",
    "\n",
    "def load_radarPass_object_pkl(pass_timestamp,data_path):\n",
    "    fname = 'radarPass_plot_'+pass_timestamp+'.pkl'\n",
    "    with open(os.path.join(data_path,'radar_passes',fname),'rb') as f:\n",
    "        radar_pass = pickle.load(f)\n",
    "    return radar_pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom radarPass class\n",
    "This class will hold all the data for each plot in a standardized format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class radarPass: \n",
    "    \"\"\"Class for the radar pass... will hold all the relevant data!\"\"\"\n",
    "    \n",
    "    def __init__(self,lon,lat,timestamp,height,radar_refl,cpr_cloud_mask):\n",
    "        \n",
    "        # Data for the full pass\n",
    "        self.cloudsat = {}\n",
    "        self.cloudsat['lon'] = lon.tolist()\n",
    "        self.cloudsat['lat'] = lat.tolist()\n",
    "        self.cloudsat['timestamp'] = timestamp.tolist()\n",
    "        self.cloudsat['radar_refl'] = radar_refl.tolist()\n",
    "        self.cloudsat['cpr_cloud_mask'] = cpr_cloud_mask.tolist()\n",
    "        self.cloudsat['height'] = height.tolist()\n",
    "        \n",
    "        # For dealing with cloud classification data\n",
    "        self.cloudsat['cloud_dict'] = {\n",
    "            '0000': (0,'None'), \n",
    "            '0001': (1,'Ci'), \n",
    "            '0010': (2,'As'), \n",
    "            '0011': (3,'Ac'),\n",
    "            '0100': (4,'St'),\n",
    "            '0101': (5,'Sc'),\n",
    "            '0110': (6,'Cu'),\n",
    "            '0111': (7,'Ns'),\n",
    "            '1000': (8,'Deep')\n",
    "        }\n",
    "        self.cloudsat['precip_dict'] = {\n",
    "            '00': (0,'no precipitation'),\n",
    "            '01': (1,'liquid precipitation'),\n",
    "            '10': (2,'solid precipitation'), \n",
    "            '11': (3,'possible drizzle')\n",
    "        }\n",
    "    \n",
    "    def add_cloudclass(self, cldclass_data, cldclass_vdata): \n",
    "        if (sum([int(a[0]==a[1]) for a in zip(self.cloudsat['lon'][:5],cldclass_vdata['Longitude'][:5])])==5 and \n",
    "            sum([int(a[0]==a[1]) for a in zip(self.cloudsat['lat'][:5],cldclass_vdata['Latitude'][:5])])==5): \n",
    "            \n",
    "            # All the cloud class and precipitation type information in stored in a binary format. \n",
    "            # Need to decode the binary to retrieve the information. \n",
    "            self.cloudsat['cloud_class'] = np.flipud(cldclass_data['cloud_scenario'].T).tolist()\n",
    "            binary_cc = [[\"{0:b}\".format(val) for val in line]  for line in \n",
    "                         self.cloudsat['cloud_class']]\n",
    "            self.cloudsat['cloud_type'] = [[self.cloudsat['cloud_dict']\n",
    "                                            [val[-5:-1]][0] for val in line] \n",
    "                                             for line in binary_cc]\n",
    "            self.cloudsat['precip_type'] = [[self.cloudsat['precip_dict']\n",
    "                                            [val[-14:-12]][0] if len(val)>12 else -1 \n",
    "                                             for val in line] for line in binary_cc]\n",
    "            \n",
    "        else: \n",
    "            # Set fields equal to None if the lat/lon data from the two files doesn't match. \n",
    "            # Means that the two files are incompatible. \n",
    "            self.cloudsat['cloud_class'] = None\n",
    "            self.cloudsat['cloud_type']  = None\n",
    "            self.cloudsat['precip_type'] = None\n",
    "            print(\"Cloud classification file doesn't match up with this radar pass.\") \n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def add_cwc(self, cwc_data, cwc_vdata): \n",
    "        if (sum([int(a[0]==a[1]) for a in zip(self.cloudsat['lon'][:5],cwc_vdata[\"Longitude\"][:5])])==5 and \n",
    "            sum([int(a[0]==a[1]) for a in zip(self.cloudsat['lat'][:5],cwc_vdata[\"Latitude\"][:5])])==5): \n",
    "            \n",
    "            self.cloudsat['lwc'] = np.flipud(cwc_data['RO_liq_water_content'].T)/1000\n",
    "            self.cloudsat['lwc_unc'] = np.flipud(cwc_data['RO_liq_water_content_uncertainty'].T).astype(float)\n",
    "            self.cloudsat['iwc'] = np.flipud(cwc_data['RO_ice_water_content'].T)/1000\n",
    "            self.cloudsat['iwc_unc'] = np.flipud(cwc_data['RO_ice_water_content_uncertainty'].T).astype(float)\n",
    "            self.cloudsat['lwc_unc'][self.cloudsat['lwc']<0] = np.nan\n",
    "            self.cloudsat['iwc_unc'][self.cloudsat['iwc']<0] = np.nan\n",
    "            self.cloudsat['lwc'][self.cloudsat['lwc']<0] = np.nan\n",
    "            self.cloudsat['iwc'][self.cloudsat['iwc']<0] = np.nan\n",
    "            \n",
    "        else: \n",
    "            self.cloudsat['lwc'] = None\n",
    "            self.cloudsat['lwc_unc'] = None\n",
    "            self.cloudsat['iwc'] = None\n",
    "            self.cloudsat['iwc_unc'] = None\n",
    "            print(\"Cloud water content file doesn't match up with this radar pass.\") \n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def trim_pass(self,area_of_interest): \n",
    "        \"\"\"Trim the radar pass so that it only contains profiles in the 'area of interest' \n",
    "        (which is given in the default_crs).\n",
    "        \"\"\"\n",
    "        \n",
    "        def determine_trim_indices(radar_pass,area_of_interest): \n",
    "            \"\"\"Determine which profile indices will be kept after the trimming.\"\"\"\n",
    "            points = [Point(lon,lat) for lon,lat in zip(radar_pass.cloudsat['lon'],\n",
    "                                                        radar_pass.cloudsat['lat'])]\n",
    "            traj_gdf = gpd.GeoDataFrame({'idx':list(range(len(radar_pass.cloudsat['lon']))),\n",
    "                                         'geometry':points})\n",
    "            traj_gdf.crs = {'init':'epsg:4326'}\n",
    "            traj_gdf = traj_gdf.to_crs({'init':default_crs})\n",
    "            traj_gdf = gpd.sjoin(traj_gdf,area_of_interest,how='inner',op='within')\\\n",
    "                          .drop('index_right',axis=1)\n",
    "            inds_to_keep = np.asarray(traj_gdf['idx'])\n",
    "            return inds_to_keep\n",
    "        \n",
    "        inds_to_keep = determine_trim_indices(self,area_of_interest)\n",
    "        \n",
    "        self.cloudsat['lon'] = (np.array(self.cloudsat['lon'])[inds_to_keep]).tolist()\n",
    "        self.cloudsat['lat'] = (np.array(self.cloudsat['lat'])[inds_to_keep]).tolist()\n",
    "        self.cloudsat['timestamp'] = (np.array(self.cloudsat['timestamp'])[inds_to_keep]).tolist()\n",
    "        self.cloudsat['radar_refl'] = (np.array(self.cloudsat['radar_refl'])[:,inds_to_keep]).tolist()\n",
    "        self.cloudsat['cpr_cloud_mask'] = (np.array(self.cloudsat['cpr_cloud_mask'])[:,inds_to_keep]).tolist()\n",
    "        # Cloud mask data\n",
    "        try: \n",
    "            self.cloudsat['cloud_class'] = (np.array(self.cloudsat['cloud_class'])[:,inds_to_keep]).tolist()\n",
    "            self.cloudsat['cloud_type'] = (np.array(self.cloudsat['cloud_type'])[:,inds_to_keep]).tolist()\n",
    "            self.cloudsat['precip_type'] = (np.array(self.cloudsat['precip_type'])[:,inds_to_keep]).tolist()\n",
    "        except: \n",
    "            self.cloudsat['cloud_class'] = None\n",
    "            self.cloudsat['cloud_type'] = None\n",
    "            self.cloudsat['precip_type'] = None\n",
    "        # Cloud water content data  \n",
    "        try: \n",
    "            self.cloudsat['lwc'] = (np.array(self.cloudsat['lwc'])[:,inds_to_keep]).tolist()\n",
    "            self.cloudsat['lwc_unc'] = (np.array(self.cloudsat['lwc_unc'])[:,inds_to_keep]).tolist()\n",
    "            self.cloudsat['iwc'] = (np.array(self.cloudsat['iwc'])[:,inds_to_keep]).tolist()\n",
    "            self.cloudsat['iwc_unc'] = (np.array(self.cloudsat['iwc_unc'])[:,inds_to_keep]).tolist()\n",
    "        except: \n",
    "            self.cloudsat['lwc'] = None\n",
    "            self.cloudsat['lwc_unc'] = None\n",
    "            self.cloudsat['iwc'] = None\n",
    "            self.cloudsat['iwc_unc'] = None\n",
    "        self.cloudsat['trim_inds'] = inds_to_keep\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def add_era5_data(self):\n",
    "        \"\"\"Adds ERA5 fields to the radar pass. The date/hour for those fields is determined by \n",
    "        rounding the median time in the satellite's pass through the Arctic to the nearest hour. \n",
    "        \n",
    "        PARAMETERS: \n",
    "        self \n",
    "        \n",
    "        RETURNS:\n",
    "        self, with one additional attribute called 'era5'. 'era5' is a dictionary that contains \n",
    "        all the era5 data, including the x coordinates, y coordinates, and atmospheric variables. \n",
    "        \"\"\"\n",
    "        \n",
    "        def find_era_date_hour(radar_pass):\n",
    "            \"\"\"Identifies the date/hour in the ERA data that's closest to the middle of \n",
    "            the satellite's pass through the Arctic.\n",
    "            \"\"\"\n",
    "            t = radar_pass.cloudsat['timestamp'][len(radar_pass.cloudsat['timestamp'])//2]\n",
    "            if t.minute >= 30:\n",
    "                rounded_time = t.replace(second=0, microsecond=0, minute=0, hour=t.hour+1)\n",
    "            else:\n",
    "                rounded_time = t.replace(second=0, microsecond=0, minute=0)\n",
    "            return datetime.datetime.strftime(rounded_time,'%Y%m%d'), rounded_time.hour\n",
    "\n",
    "        \n",
    "        def open_atmospheric_vbls_file(radar_pass):\n",
    "            \"\"\"Opens ERA5 'atmospheric variables' file that corresponds to the given radar pass. \n",
    "            Returns the file handle and the hour of data in the file that corresponds \n",
    "            to the radar pass. \n",
    "            \"\"\"\n",
    "            dstr,hr = find_era_date_hour(radar_pass)\n",
    "            file = 'ERA5_atmospheric_vbls_'+dstr+'.nc'\n",
    "            atm_dataset = Dataset(os.path.join(data_path,'ERA5',file))\n",
    "\n",
    "            return atm_dataset,hr\n",
    "\n",
    "        \n",
    "        def open_singlelevel_vbls_file(radar_pass): \n",
    "            \"\"\"Opens ERA5 'single level variables' file that corresponds to the given radar pass. \n",
    "            Returns the file handle and the hour of data in the file that corresponds \n",
    "            to the radar pass. \n",
    "            \"\"\"\n",
    "            dstr,hr = find_era_date_hour(radar_pass)\n",
    "            file = 'ERA5_singlelevel_vbls_'+dstr+'.nc'\n",
    "            slev_dataset = Dataset(os.path.join(data_path,'ERA5',file))\n",
    "\n",
    "            return slev_dataset,hr\n",
    "        \n",
    "        \n",
    "        def reproject_from_epsg4326(lon,lat,field,dst_crs):\n",
    "            \"\"\"Reproject ERA5 field from lat/lon coordinates to specified destination crs.\"\"\"\n",
    "            # Parameters needed for the transformation. \n",
    "            width = field.shape[1]\n",
    "            height = field.shape[0]\n",
    "            left,bottom,right,top = lon[0],lat[-1],lon[-1],lat[0]\n",
    "            src_crs = {'init':'epsg:4326'}\n",
    "\n",
    "            # Calculate affine transformation matrix for the source field. \n",
    "            src_transform = from_bounds(left, bottom, right, top, width, height)\n",
    "\n",
    "            # Calculate affine transformation matrix, width, and height for the destination field.\n",
    "            dst_transform, dst_width, dst_height = calculate_default_transform(src_crs, dst_crs, width, \n",
    "                                                                               height, left = left, \n",
    "                                                                               bottom = bottom, \n",
    "                                                                               right = right, top = top)\n",
    "\n",
    "            # Perform reprojection. \n",
    "            destination_array = np.zeros((dst_height,dst_width))\n",
    "            reproject(source=field, destination=destination_array, src_transform=src_transform,\n",
    "                      src_crs=src_crs, dst_transform=dst_transform, \n",
    "                      dst_crs=dst_crs, resampling=Resampling.nearest)\n",
    "\n",
    "            # Replace fill values with NaN's. \n",
    "            destination_array[destination_array==1e20] = np.NaN\n",
    "\n",
    "            # Get X and Y vectors for the transformed field. I could be doing this wrong, \n",
    "            # but my method definitely works for this specific Arctic case! \n",
    "            dst_x = np.linspace(dst_transform[2],dst_transform[5],dst_width)\n",
    "            dst_y = np.linspace(dst_transform[2],dst_transform[5],dst_height)\n",
    "\n",
    "            return dst_x,dst_y,destination_array\n",
    "\n",
    "        \n",
    "        def get_lat_lon_level_hours(atm_dataset): \n",
    "            \"\"\"Extracts latitude, longitude, time, and level vectors from netCDF dataset.\"\"\"\n",
    "            lat = atm_dataset.variables['latitude'][:]\n",
    "            lon = atm_dataset.variables['longitude'][:]\n",
    "            hours = np.array([(pd.Timestamp('19000101')+(hrs_since*pd.Timedelta('1 hours'))).hour \n",
    "                              for hrs_since in atm_dataset.variables['time'][:]])\n",
    "            if 'level' in atm_dataset.variables.keys(): \n",
    "                levels = atm_dataset.variables['level'][:]\n",
    "            else: \n",
    "                levels = None\n",
    "            return lat,lon,hours,levels\n",
    "\n",
    "\n",
    "        def get_weather_variable_names(atm_dataset):\n",
    "            \"\"\"Retrieves names of netCDF variables that correspond to weather variables (and \n",
    "            therefore aren't the latitude, longitude, atmospheric level, or time).\n",
    "            \"\"\"\n",
    "            return [var for var in atm_dataset.variables.keys() if var not in \n",
    "                   ['longitude', 'latitude', 'level', 'time']]\n",
    "\n",
    "        \n",
    "        def add_era5_atmospheric_vbls(atm_dataset,hr,era5={}):\n",
    "            \"\"\"Adds ERA5 variables that are only available at atmospheric levels \n",
    "            to the 'era5' dictionary.\n",
    "            \"\"\"\n",
    "\n",
    "            lat,lon,hours,levels = get_lat_lon_level_hours(atm_dataset)\n",
    "            wx_vbls = get_weather_variable_names(atm_dataset)\n",
    "\n",
    "            for vbl in wx_vbls: \n",
    "                for level in levels:\n",
    "                    i_time = np.where(hours==hr)[0][0]\n",
    "                    i_level = np.where(levels==level)[0][0]\n",
    "                    field = atm_dataset.variables[vbl][i_time,i_level,:,:]\n",
    "                    reproj_x,reproj_y,reproj_field = reproject_from_epsg4326(lon,lat,field,\n",
    "                                                                             {'init':default_crs})\n",
    "                    if vbl in ['q','clwc','ciwc']: # Change units \n",
    "                        reproj_field = reproj_field*1000\n",
    "                    elif vbl == 'z': \n",
    "                        reproj_field = reproj_field/9.81\n",
    "                        \n",
    "                    era5[vbl+'_'+str(level)] = np.flipud(reproj_field).tolist()\n",
    "            \n",
    "            # Convert vertical velocities to mm/s\n",
    "            era5['w_700'] = (-np.array(era5['w_700'])*287*np.array(era5['t_700'])/(7e4*9.81)*1000).tolist()\n",
    "            era5['w_850'] = (-np.array(era5['w_850'])*287*np.array(era5['t_850'])/(8.5e4*9.81)*1000).tolist()\n",
    "            \n",
    "            era5['x'],era5['y'] = reproj_x, reproj_y\n",
    "            \n",
    "            return era5\n",
    "\n",
    "        \n",
    "        def add_era5_singlelevel_vbls(slev_dataset,hr,era5={}):\n",
    "            \"\"\"Adds ERA5 variables that are only available at the surface to the 'era5' \n",
    "            dictionary.\n",
    "            \"\"\"\n",
    "\n",
    "            lat,lon,hours,_ = get_lat_lon_level_hours(slev_dataset)\n",
    "            wx_vbls = get_weather_variable_names(slev_dataset)\n",
    "\n",
    "            for vbl in wx_vbls: \n",
    "                i_time = np.where(hours==hr)[0][0]\n",
    "                field = slev_dataset.variables[vbl][i_time,:,:]\n",
    "                reproj_x,reproj_y,reproj_field = reproject_from_epsg4326(lon,lat,field,\n",
    "                                                                         {'init':default_crs})\n",
    "                if vbl in ['msl','sp']: # Change units\n",
    "                    reproj_field = reproj_field/100\n",
    "                        \n",
    "                era5[vbl] = np.flipud(reproj_field).tolist()\n",
    "\n",
    "            if 'x' not in era5.keys() and 'y' not in era5.keys(): \n",
    "                era5['x'],era5['y'] = reproj_x.tolist(), reproj_y.tolist()\n",
    "\n",
    "            return era5\n",
    "\n",
    "        atm_dataset,hr = open_atmospheric_vbls_file(self)\n",
    "        era5 = add_era5_atmospheric_vbls(atm_dataset,hr,era5={})\n",
    "        slev_dataset,hr = open_singlelevel_vbls_file(self)\n",
    "        era5 = add_era5_singlelevel_vbls(slev_dataset,hr,era5=era5)\n",
    "        self.era5 = era5\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_json_serializable_obj(self):\n",
    "        \"\"\"Convert structure and all data fields to dictionaries and lists.\"\"\"\n",
    "        radar_dict = self.__dict__\n",
    "        for key,val in radar_dict['cloudsat'].items(): \n",
    "            if isinstance(val,np.ndarray): \n",
    "                radar_dict['cloudsat'][key] = val.tolist()\n",
    "        for key,val in radar_dict['era5'].items():\n",
    "            if isinstance(val,np.ndarray): \n",
    "                radar_dict['era5'][key] = val.tolist()    \n",
    "        return radar_dict\n",
    "    \n",
    "    def reduce_size_cloudsat(self, reduction_factor=3, \n",
    "                             trim_vbls=['cpr_cloud_mask','cloud_class','precip_type','lwc_unc','iwc_unc']): \n",
    "        \"\"\"Reduce image sizes by taking every reduction_factor'th profile. Also drop some fields that\n",
    "        are less useful.\n",
    "        \"\"\"\n",
    "        \n",
    "        reduce_1d = ['lon','lat','timestamp']\n",
    "        for var in reduce_1d: \n",
    "            self.cloudsat[var] = (np.array(self.cloudsat[var])[::reduction_factor]).tolist()\n",
    "\n",
    "        reduce_2d = ['radar_refl','cpr_cloud_mask','cloud_class','cloud_type','precip_type',\n",
    "                     'lwc','lwc_unc','iwc','iwc_unc']\n",
    "        for var in reduce_2d: \n",
    "            self.cloudsat[var] = (np.array(self.cloudsat[var])[:,::reduction_factor]).tolist()\n",
    "            \n",
    "        for vbl in trim_vbls: \n",
    "            del(self.cloudsat[vbl])\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def reduce_size_era5(self, reduction_factor=3, trim_vbls=['d2m','sp','z_850','z_700']): \n",
    "        \"\"\"Reduce image sizes by taking every reduction_factor'th pixel in both the x and y directions. \n",
    "        Also drop some fields that are less useful.\n",
    "        \"\"\"\n",
    "        reduce_1d = ['x','y']\n",
    "        for var in reduce_1d: \n",
    "            self.era5[var] = (np.array(self.era5[var])[::reduction_factor]).tolist()\n",
    "        \n",
    "        reduce_2d = [var for var in radar_pass.era5.keys() if var not in reduce_1d]\n",
    "        for var in reduce_2d: \n",
    "            self.era5[var] = (np.array(self.era5[var])[::reduction_factor,::reduction_factor]).tolist()\n",
    "        \n",
    "        for vbl in trim_vbls: \n",
    "            del(self.era5[vbl]) \n",
    "        \n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create radarPass instances for each CloudSat file\n",
    "* Initialize the `radarPass` instance using the GEOPROF file for the given radar pass. \n",
    "  * Good tutorial for loading CloudSat data is available [here](https://www.science-emergence.com/Articles/How-to-read-CloudSat-2B-GEOPROF-GRANULE-HDF4-file-using-python-and-pyhdf-/). \n",
    "* Add cloud classification data to `radarPass` instance. \n",
    "* Add cloud water content data to `radarPass` instance. \n",
    "* Trim the `radarPass` instance so that it only contains radar profiles in the area of interest. \n",
    "* Add ERA5 data to the `radarPass` instance. \n",
    "  * Time: the hour closest to the median time in the trimmed `radarPass`. \n",
    "  * Location: (N,W,S,E) = (90,-180,60,180) (everywhere above 60N)\n",
    "  * Fields: \n",
    "    * Temperature at 850mb, 700mb\n",
    "    * Specific humidity at 850mb, 700mb\n",
    "    * Specific cloud liquid water content at 850mb, 700mb\n",
    "    * Specific cloud ice water content at 850mb, 700mb\n",
    "    * Vertical velocity at 850mb, 700mb\n",
    "    * Geopotential at 850mb, 700mb\n",
    "    * U component of wind at 850mb, 700mb\n",
    "    * V component of wind at 850mb, 700mb\n",
    "    * Surface pressure\n",
    "    * Mean sea level pressure\n",
    "    * 2 metre temperature\n",
    "    * 2 metre dewpoint temperature\n",
    "* Then create reduced instance of radar_pass class, which has lower resolution and only the necessary CloudSat and ERA5 fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify area of interest: (N, E, S, W) = (3e6, 3e6, -3e6, -3e6) in EPSG:3995\n",
    "area_of_interest = specify_area_of_interest_EPSG3995()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015244002319_49702\n",
      "2015244020212_49703\n",
      "2015244034105_49704\n",
      "2015244051958_49705\n",
      "2015244065851_49706\n",
      "2015244083744_49707\n",
      "2015244101637_49708\n",
      "2015244115530_49709\n",
      "2015244133423_49710\n",
      "2015244151316_49711\n"
     ]
    }
   ],
   "source": [
    "# Get original ID numbers for all the downloaded CloudSat files. \n",
    "pass_ids = get_all_downloaded_pass_ids(data_path)\n",
    "\n",
    "for pass_id in pass_ids[:10]: \n",
    "    \n",
    "    print(pass_id)\n",
    "    \n",
    "    # Create full radarPass object\n",
    "    radar_pass_f = create_radarPass_instance(data_path,pass_id)\n",
    "    radar_pass_f = add_cldclass_to_radarPass(radar_pass_f,data_path,pass_id)\n",
    "    radar_pass_f = add_cwc_to_radarPass(radar_pass_f,data_path,pass_id)\n",
    "    radar_pass_f = radar_pass_f.trim_pass(area_of_interest)\n",
    "    radar_pass_f = radar_pass_f.add_era5_data()\n",
    "\n",
    "    # Create smaller version: reduce resolution, remove some fields that are less useful. \n",
    "    radar_pass = copy.deepcopy(radar_pass_f)\n",
    "    radar_pass = radar_pass.reduce_size_cloudsat(reduction_factor=3)\n",
    "    radar_pass = radar_pass.reduce_size_era5(reduction_factor=4)\n",
    "    pass_timestamp = save_radarPass_object_pkl(radar_pass,data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and plot settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_radar_trajectory(radar_pass):\n",
    "    \"\"\"Returns a dataframe with radar trajectory in EPSG:3995\"\"\"\n",
    "    points = [Point(lon,lat) for lon,lat in zip(radar_pass.cloudsat['lon'],radar_pass.cloudsat['lat'])]\n",
    "    traj_gdf = gpd.GeoDataFrame({'t':radar_pass.cloudsat['timestamp'],'lat':radar_pass.cloudsat['lat'],\n",
    "                                 'lon':radar_pass.cloudsat['lon'],'geometry':points})\n",
    "    traj_gdf.crs = {'init':'epsg:4326'}\n",
    "    traj_gdf = traj_gdf.to_crs({'init':default_crs})\n",
    "    traj_gdf['x'] = [pt.coords.xy[0][0] for pt in traj_gdf['geometry']]\n",
    "    traj_gdf['y'] = [pt.coords.xy[1][0] for pt in traj_gdf['geometry']]\n",
    "    traj_df = pd.DataFrame(traj_gdf.drop('geometry',axis=1)).iloc[::12]\n",
    "    traj_df['t_str'] = traj_df['t'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return traj_df\n",
    "\n",
    "\n",
    "def prepare_image_for_plot(image,min_val,fill_min,max_val,fill_max): \n",
    "    \"\"\"Establishes bounds in the image values. Replaces values above and below those \n",
    "    bounds with specified fill values.\n",
    "    \"\"\"\n",
    "    image = np.array(image)\n",
    "    image[image<=min_val] = fill_min\n",
    "    image[image>=max_val] = fill_max\n",
    "    image = image.tolist()\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_hex_matplotlib_cmap(cmap_name,num_steps,reverse=False): \n",
    "    \"\"\"Creates bokeh-compatible color maps (lists of hex colors) from standard Matplotlib colormaps.\"\"\"\n",
    "    from matplotlib import cm\n",
    "    from matplotlib.colors import rgb2hex\n",
    "    cmap = cm.get_cmap(cmap_name, num_steps)\n",
    "    hex_map = [rgb2hex(cmap(i)[:3]) for i in range(num_steps)]\n",
    "    if reverse: \n",
    "        hex_map = hex_map[::-1]\n",
    "    return hex_map\n",
    "\n",
    "\n",
    "def find_corresponding_pass(entered_timestamp,data_path):  \n",
    "    \"\"\"Returns timestamp of radar pass that occurs closest to the entered time.\"\"\"\n",
    "    pass_times_all = sorted([pd.Timestamp(''.join(fname.split('_')[2:4])[:-4]) for fname in \n",
    "                             os.listdir(os.path.join(data_path,'radar_passes')) if fname[0]!='.'])\n",
    "    pass_time = pass_times_all[np.argmin(np.abs(np.array(pass_times_all)-entered_timestamp))]\n",
    "    pass_timestamp = pass_time.strftime('%Y%m%d_%H%M%S')\n",
    "    return pass_timestamp\n",
    "\n",
    "\n",
    "def create_CDS_land(area_of_interest): \n",
    "    countries = load_country_geometries(area_of_interest)\n",
    "    x_land_coords,y_land_coords = prepare_polygon_coords_for_bokeh(countries)\n",
    "    land_src = ColumnDataSource({'x':x_land_coords,'y':y_land_coords})\n",
    "    return land_src\n",
    "\n",
    "\n",
    "def create_CDS_satellite_trajectory(radar_pass): \n",
    "    traj_df = get_radar_trajectory(radar_pass)\n",
    "    traj_src = ColumnDataSource(traj_df)\n",
    "    return traj_df,traj_src\n",
    "\n",
    "\n",
    "def create_CDS_era5_field(radar_pass,era5_vbl): \n",
    "    era_img_src = ColumnDataSource({'image':[np.array(radar_pass.era5[era5_vbl]).tolist()]})\n",
    "    return era_img_src\n",
    "\n",
    "\n",
    "def create_map_panel(radar_pass, era5_vbl, era_img_src, land_src, traj_src, \n",
    "                     era5_color_maps, era5_color_bounds, era5_full_names): \n",
    "\n",
    "    # Create color mapper for ERA5 field. \n",
    "    color_mapper = LinearColorMapper(palette = era5_color_maps[era5_vbl], \n",
    "                                     low = era5_color_bounds[era5_vbl][0], \n",
    "                                     high = era5_color_bounds[era5_vbl][1])\n",
    "\n",
    "    # Put the figure together. \n",
    "    pass_time = radar_pass.cloudsat['timestamp'][len(radar_pass.cloudsat['timestamp'])//2]\\\n",
    "                          .strftime('%Y-%m-%d %H:%M:%S')\n",
    "    p1 = figure(title=\"ERA5 FIELD: \"+era5_full_names[era5_vbl]+\". TRAJECTORY TIME: \"+\n",
    "                pass_time+\".\", toolbar_location=\"right\", \n",
    "                plot_width=800, plot_height=550,x_range=(-3e6, 3e6), \n",
    "                y_range=(-3e6, 3e6))\n",
    "    era_im = p1.image('image', source=era_img_src, x=radar_pass.era5['x'][0], \n",
    "                      y=radar_pass.era5['y'][0], \n",
    "                      dw=radar_pass.era5['x'][-1]-radar_pass.era5['x'][0], \n",
    "                      dh=radar_pass.era5['y'][-1]-radar_pass.era5['y'][0], \n",
    "                      color_mapper=color_mapper)\n",
    "    era_im.glyph.color_mapper.nan_color = (1, 1, 1, 0.1)\n",
    "    pr_land = p1.patches('x', 'y', source=land_src, fill_color='black', fill_alpha=0.0, \n",
    "                         line_color=\"black\", line_width=0.8)\n",
    "    lr_traj = p1.line('x','y',source=traj_src,line_color='black',line_width=2)\n",
    "    cr_traj = p1.circle('x','y',source=traj_src,fill_color='black',fill_alpha=0.3,\n",
    "                        line_color='black',line_alpha=0,hover_fill_color='green',\n",
    "                        hover_fill_alpha=1,hover_line_color='black',hover_line_alpha=1,\n",
    "                        size=5)\n",
    "\n",
    "    # Formatting. \n",
    "    p1.xaxis.major_label_text_font_size='9pt'\n",
    "    p1.yaxis.major_label_text_font_size='9pt'\n",
    "\n",
    "    # Add colorbar.  \n",
    "    color_bar = ColorBar(color_mapper=color_mapper, bar_line_color='black',\n",
    "                         major_tick_line_color='black',label_standoff=6, \n",
    "                         border_line_color=None, location=(0,0))\n",
    "    p1.add_layout(color_bar, 'right')\n",
    "    \n",
    "    return p1, cr_traj\n",
    "\n",
    "\n",
    "def create_CDS_satellite_position(traj_df): \n",
    "    sat_src = ColumnDataSource({'t':[min(traj_df['t']),min(traj_df['t'])],'y':[-5000,25000]})\n",
    "    return sat_src\n",
    "\n",
    "\n",
    "def create_CDS_satellite_image(radar_pass,cloudsat_vbl): \n",
    "    if cloudsat_vbl == 'radar_refl': \n",
    "        img = prepare_image_for_plot(radar_pass.cloudsat['radar_refl'].copy(),-2300,np.nan,1500,1500)\n",
    "    elif cloudsat_vbl == 'cloud_type':\n",
    "        img = prepare_image_for_plot(radar_pass.cloudsat['cloud_type'].copy(),0,0,10,10)\n",
    "    elif cloudsat_vbl == 'cpr_cloud_mask': \n",
    "        img = prepare_image_for_plot(radar_pass.cloudsat['cpr_cloud_mask'].copy(),0,0,100,100)\n",
    "    elif cloudsat_vbl == 'iwc': \n",
    "        img = prepare_image_for_plot(radar_pass.cloudsat['iwc'].copy(),0,0,4000,4000)\n",
    "    elif cloudsat_vbl == 'lwc': \n",
    "        img = prepare_image_for_plot(radar_pass.cloudsat['lwc'].copy(),0,0,4000,4000)\n",
    "    elif cloudsat_vbl == 'iwc_unc': \n",
    "        img = prepare_image_for_plot(radar_pass.cloudsat['iwc_unc'].copy(),0,0,4000,4000)\n",
    "    elif cloudsat_vbl == 'lwc_unc':\n",
    "        img = prepare_image_for_plot(radar_pass.cloudsat['lwc_unc'].copy(),0,0,4000,4000)\n",
    "    sat_img_src = ColumnDataSource({'image':[img]})\n",
    "    return sat_img_src\n",
    "\n",
    "\n",
    "def create_satellite_panel(radar_pass, cloudsat_vbl, sat_img_src, sat_src, \n",
    "                           traj_df, radar_full_names): \n",
    "    \n",
    "    # Create color mapper. \n",
    "    if cloudsat_vbl == 'radar_refl': \n",
    "        color_mapper = LinearColorMapper(palette='Plasma256',low=-2300,high=1500)\n",
    "    elif cloudsat_vbl == 'cloud_type':\n",
    "        color_mapper = LinearColorMapper(palette='Paired9',low=-0.5,high=8.5)\n",
    "    elif cloudsat_vbl == 'cpr_cloud_mask': \n",
    "        color_mapper = LinearColorMapper(palette='Paired11',low=-2,high=42)\n",
    "    elif cloudsat_vbl == 'iwc': \n",
    "        cmp = get_hex_matplotlib_cmap('Blues',100,reverse=False)\n",
    "        cmp.insert(0,'#FFFFFF')\n",
    "        color_mapper = LinearColorMapper(palette=cmp,low=0,high=0.25)\n",
    "    elif cloudsat_vbl == 'lwc': \n",
    "        cmp = get_hex_matplotlib_cmap('Greens',100,reverse=False)\n",
    "        cmp.insert(0,'#FFFFFF')\n",
    "        color_mapper = LinearColorMapper(palette=cmp,low=0,high=1)\n",
    "    elif cloudsat_vbl == 'iwc_unc': \n",
    "        color_mapper = LinearColorMapper(palette='Viridis256',low=0,high=200)\n",
    "    elif cloudsat_vbl == 'lwc_unc': \n",
    "        color_mapper = LinearColorMapper(palette='Viridis256',low=0,high=200)\n",
    "\n",
    "    # Put the plot together. \n",
    "    p2 = figure(title=\"CloudSat: \"+radar_full_names[cloudsat_vbl], toolbar_location=\"right\",\n",
    "                plot_width=800, plot_height=250, active_scroll = \"wheel_zoom\",\n",
    "                x_range=(min(traj_df['t']), max(traj_df['t'])),y_range=(0, 15000))\n",
    "    sat_im = p2.image('image', source=sat_img_src, x=min(traj_df['t']), \n",
    "                      y=radar_pass.cloudsat['height'][0], dw=max(traj_df['t'])-min(traj_df['t']), \n",
    "                      dh=(radar_pass.cloudsat['height'][-1]-radar_pass.cloudsat['height'][0]), \n",
    "                      color_mapper=color_mapper)\n",
    "    sat_im.glyph.color_mapper.nan_color = (0, 0, 0, 1)\n",
    "    lr_sat = p2.line('t','y',source=sat_src,line_color='gray',line_width=3)\n",
    "\n",
    "    # Add colorbar. \n",
    "    #if cloudsat_vbl == 'radar_refl': \n",
    "    #    ticker = FixedTicker(ticks=np.linspace(-2300,1500,11))\n",
    "    #elif cloudsat_vbl == 'cloud_type': \n",
    "    #    ticker = FixedTicker(ticks=[i for i in range(9)])\n",
    "    #elif cloudsat_vbl == 'cpr_cloud_mask':\n",
    "    #    ticker = FixedTicker(ticks=[4*i for i in range(11)])\n",
    "    color_bar = ColorBar(color_mapper=color_mapper, bar_line_color='black',\n",
    "                         major_tick_line_color='black',label_standoff=8, \n",
    "                         border_line_color=None, location=(0,0))\n",
    "    p2.add_layout(color_bar, 'right')\n",
    "\n",
    "    # Adjust some formatting. \n",
    "    p2.xaxis.axis_label = 'Time (UTC)'\n",
    "    p2.xaxis.axis_label_text_font_size='11pt'\n",
    "    p2.xaxis.axis_label_text_font_style='normal'\n",
    "    p2.xaxis.major_label_text_font_size='9pt'\n",
    "    p2.yaxis.axis_label = 'Height (m)'\n",
    "    p2.yaxis.axis_label_text_font_size='11pt'\n",
    "    p2.yaxis.axis_label_text_font_style='normal'\n",
    "    p2.yaxis.major_label_text_font_size='9pt'\n",
    "\n",
    "    # Create datetime x axis\n",
    "    p2.xaxis.formatter = DatetimeTickFormatter(\n",
    "        hours=[\"%H:%M:%S\"],\n",
    "        minutes=[\"%H:%M:%S\"],\n",
    "        seconds=[\"%H:%M:%S\"],\n",
    "    )\n",
    "\n",
    "    return p2, sat_src\n",
    "\n",
    "\n",
    "def add_hovertool(p1, cr_traj, traj_src, sat_src, traj_df): \n",
    "    \n",
    "    # Create the JS callback for vertical line on radar plots. \n",
    "    callback_htool = CustomJS(args={'traj_src':traj_src,'sat_src':sat_src}, code=\"\"\"\n",
    "        const indices = cb_data.index[\"1d\"].indices[0];\n",
    "\n",
    "        var data_traj = traj_src.data\n",
    "        var t_traj = data_traj['t']\n",
    "        const t_val = t_traj[indices]\n",
    "\n",
    "        var data_sat = sat_src.data;\n",
    "        var t_sat = data_sat['t']\n",
    "        t_sat[0] = t_val\n",
    "        t_sat[1] = t_val\n",
    "        sat_src.change.emit(); \n",
    "    \"\"\")\n",
    "\n",
    "    # Add the hovertool for the satellite trajectory points on top panel, which are \n",
    "    # linked to the vertical line on the bottom panel. \n",
    "    htool_mode = ('vline' if max(traj_df['y'])-min(traj_df['y'])<=\n",
    "                              (max(traj_df['x'])-min(traj_df['x'])) else 'hline')\n",
    "    tooltips1 = [(\"lat\", \"@lat\"),(\"lon\", \"@lon\"),('time','@t_str')]\n",
    "    p1.add_tools(HoverTool(renderers=[cr_traj],callback=callback_htool,\n",
    "                           mode=htool_mode,tooltips=tooltips1))\n",
    "    \n",
    "    return p1\n",
    "\n",
    "\n",
    "era5_color_bounds = {'q_700':[0,4],\n",
    "                  'q_850':[0,6],\n",
    "                  'z_700':[2500,3300],\n",
    "                  'z_850':[1000,1700],\n",
    "                  'ciwc_850':[0,0.1],\n",
    "                  'ciwc_700':[0,0.1],\n",
    "                  'clwc_850':[0,1],\n",
    "                  'clwc_700':[0,1],\n",
    "                  't_850':[230,316], \n",
    "                  't_700':[230,316],\n",
    "                  'u_850':[-25,25],\n",
    "                  'u_700':[-25,25],\n",
    "                  'v_850':[-25,25],\n",
    "                  'v_700':[-25,25],\n",
    "                  'w_850':[-50,50],\n",
    "                  'w_700':[-50,50],\n",
    "                  'd2m':[230,316], \n",
    "                  't2m':[230,316], \n",
    "                  'msl':[960,1065], \n",
    "                  'sp':[650,1065]}\n",
    "\n",
    "era5_color_maps = {'q_700':get_hex_matplotlib_cmap('BrBG',15,reverse=False), #r\n",
    "                  'q_850':get_hex_matplotlib_cmap('BrBG',15,reverse=False), #r\n",
    "                  'z_700':get_hex_matplotlib_cmap('PuOr',25,reverse=True),\n",
    "                  'z_850':get_hex_matplotlib_cmap('PuOr',25,reverse=True),\n",
    "                  'ciwc_850':get_hex_matplotlib_cmap('Blues',11,reverse=False),\n",
    "                  'ciwc_700':get_hex_matplotlib_cmap('Blues',11,reverse=False),\n",
    "                  'clwc_850':get_hex_matplotlib_cmap('Greens',11,reverse=False),\n",
    "                  'clwc_700':get_hex_matplotlib_cmap('Greens',11,reverse=False),\n",
    "                  't_850':get_hex_matplotlib_cmap('bwr',25,reverse=False), \n",
    "                  't_700':get_hex_matplotlib_cmap('bwr',25,reverse=False),\n",
    "                  'u_850':get_hex_matplotlib_cmap('PRGn',15,reverse=False),\n",
    "                  'u_700':get_hex_matplotlib_cmap('PRGn',15,reverse=False),\n",
    "                  'v_850':get_hex_matplotlib_cmap('PRGn',15,reverse=False),\n",
    "                  'v_700':get_hex_matplotlib_cmap('PRGn',15,reverse=False),\n",
    "                  'w_850':get_hex_matplotlib_cmap('PRGn',15,reverse=False),\n",
    "                  'w_700':get_hex_matplotlib_cmap('PRGn',15,reverse=False),\n",
    "                  'd2m':get_hex_matplotlib_cmap('BrBG',25,reverse=False), \n",
    "                  't2m':get_hex_matplotlib_cmap('bwr',25,reverse=False), \n",
    "                  'msl':get_hex_matplotlib_cmap('PuOr',50,reverse=True), \n",
    "                  'sp':get_hex_matplotlib_cmap('PuOr',50,reverse=True)}\n",
    "\n",
    "era5_color_maps['ciwc_850'].insert(0,'#FFFFFF')\n",
    "era5_color_maps['clwc_850'].insert(0,'#FFFFFF')\n",
    "era5_color_maps['ciwc_700'].insert(0,'#FFFFFF')\n",
    "era5_color_maps['clwc_700'].insert(0,'#FFFFFF')\n",
    "\n",
    "era5_full_names = {'q_700':'Specific humidity, 700 mb (g/kg)',\n",
    "                  'q_850':'Specific humidity, 850 mb (g/kg)',\n",
    "                  'z_700':'Geopotential height, 700 mb (m)', \n",
    "                  'z_850':'Geopotential height, 850 mb (m)',\n",
    "                  'ciwc_850':'Cloud ice water content, 850 mb (g/kg)',\n",
    "                  'ciwc_700':'Cloud ice water content, 700 mb (g/kg)',\n",
    "                  'clwc_850':'Cloud liquid water content, 850 mb (g/kg)',\n",
    "                  'clwc_700':'Cloud liquid water content, 700 mb (g/kg)',\n",
    "                  't_850':'Temperature, 850 mb (K)', \n",
    "                  't_700':'Temperature, 700 mb (K)',\n",
    "                  'u_850':'E/W Wind, 850 mb (m/s)',\n",
    "                  'u_700':'E/W Wind, 700 mb (m/s)',\n",
    "                  'v_850':'N/S Wind, 850 mb (m/s)',\n",
    "                  'v_700':'N/S Wind, 700 mb (m/s)',\n",
    "                  'w_850':'Vertical velocity, 850 mb (mm/s)',\n",
    "                  'w_700':'Vertical velocity, 700 mb (mm/s)',\n",
    "                  'd2m':'Dewpoint at surface (K)', \n",
    "                  't2m':'Temperature at 2 m (K)', \n",
    "                  'msl':'Mean sea level pressure (mb)', \n",
    "                  'sp':'Surface pressure (mb)'}\n",
    "\n",
    "cloud_dict = {\n",
    "    '0000': (0,'None'), \n",
    "    '0001': (1,'Ci'), \n",
    "    '0010': (2,'As'), \n",
    "    '0011': (3,'Ac'),\n",
    "    '0100': (4,'St'),\n",
    "    '0101': (5,'Sc'),\n",
    "    '0110': (6,'Cu'),\n",
    "    '0111': (7,'Ns'),\n",
    "    '1000': (8,'Deep')\n",
    "}\n",
    "\n",
    "cloud_type_title_key = ', '.join([str(a[0])+'-'+a[1] for a in \n",
    "                                  cloud_dict.values()])\n",
    "radar_full_names = {'radar_refl':'Radar Reflectivity',\n",
    "                    'cloud_type':'Cloud Type, Key: '+cloud_type_title_key,\n",
    "                    'cpr_cloud_mask':'Radar Cloud Mask',\n",
    "                    'iwc':'Cloud Ice Water Content (g/m3)',\n",
    "                    'lwc':'Cloud Liquid Water Content (g/m3)',\n",
    "                    'iwc_unc':'Cloud Ice Water Content Uncertainty (%)',\n",
    "                    'lwc_unc':'Cloud Liquid Water Content Uncertainty (%)'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the plot! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2012-01-01 10:00:00')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input variables\n",
    "query_date = '2015-09-01'\n",
    "query_hour = '11'\n",
    "era5_vbl = 'q_850'\n",
    "cloudsat_vbl = 'radar_refl'\n",
    "\n",
    "entered_time = pd.Timestamp('2015-09-01')+(int(query_hour)*pd.Timedelta('1 hours'))\n",
    "\n",
    "# Load the corrsponding radarPass instance. \n",
    "pass_timestamp = find_corresponding_pass(entered_time,data_path)\n",
    "radar_pass = load_radarPass_object_pkl(pass_timestamp,data_path)\n",
    "\n",
    "# Top panel: map of Arctic with ERA5 field. \n",
    "land_src = create_CDS_land(area_of_interest)\n",
    "traj_df,traj_src = create_CDS_satellite_trajectory(radar_pass)\n",
    "era_img_src = create_CDS_era5_field(radar_pass,era5_vbl)\n",
    "p1, cr_traj = create_map_panel(radar_pass, era5_vbl, era_img_src, land_src, traj_src, \n",
    "                               era5_color_maps, era5_color_bounds, era5_full_names)\n",
    "\n",
    "# Bottom panel: radar profiles. \n",
    "sat_src = create_CDS_satellite_position(traj_df)\n",
    "sat_img_src = create_CDS_satellite_image(radar_pass,cloudsat_vbl)\n",
    "p2, sat_src = create_satellite_panel(radar_pass, cloudsat_vbl, sat_img_src, sat_src, \n",
    "                                     traj_df, radar_full_names) \n",
    "p1 = add_hovertool(p1, cr_traj, traj_src, sat_src, traj_df)\n",
    "\n",
    "layout = gridplot([[p1],[p2]])\n",
    "show(layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still to do: \n",
    "* Run through all radar passes, save radarPass instances radarPass_YYYYmmdd_HHMMSS.pkl with only trimmed fields (to save space). \n",
    "* Create web application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.resources import CDN\n",
    "from bokeh.embed import file_html\n",
    "\n",
    "html = file_html(layout, CDN, \"My Plot\")\n",
    "\n",
    "with open(os.path.join(figs_path,'plot1.html'),'w') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
